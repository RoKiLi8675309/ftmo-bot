# =============================================================================
# FILENAME: engines/live/predictor.py
# ENVIRONMENT: Linux/WSL2 (Python 3.11)
# PATH: engines/live/predictor.py
# DEPENDENCIES: shared, river, numpy
# DESCRIPTION: Online Learning Kernel. Manages Ensemble Models (Bagging ARF),
# Feature Engineering, Labeling (Adaptive Triple Barrier), and Weighted Learning.
#
# PHOENIX STRATEGY V7.5 (SNIPER PROTOCOL - LIVE):
# 1. MEAN REVERSION PURGED: Removed Regime C.
# 2. AGGRESSOR LOGIC ALIGNMENT:
#    - REGIME FILTER: KER > 0.2 AND Choppiness < 50.0.
#    - FUEL GAUGE: RVOL > 1.5.
#    - TRIGGER: Order Flow Imbalance (Buy > 1.2x Sell) + PA Aggressor > 0.55.
# 3. SNIPER FILTERS:
#    - TREND: SMA 200 Filter.
#    - EXTENSION: RSI Filter.
# =============================================================================
import logging
import pickle
import os
import json
import time
import math
import pytz 
from datetime import datetime
from collections import defaultdict, deque, Counter
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

# Third-Party ML Imports
try:
    from river import forest, compose, preprocessing, metrics, drift, linear_model, multioutput, ensemble
except ImportError:
    print("CRITICAL: 'river' library not found. Install with: pip install river>=0.21.0")
    import sys
    sys.exit(1)

# Shared Imports
from shared import (
    CONFIG,
    LogSymbols,
    OnlineFeatureEngineer,
    AdaptiveTripleBarrier,
    ProbabilityCalibrator,
    VolumeBar,
    RiskManager
)

# New Feature Import
from shared.financial.features import MetaLabeler

logger = logging.getLogger("Predictor")

class Signal:
    """
    Represents a trading decision generated by the model.
    """
    def __init__(self, symbol: str, action: str, confidence: float, meta_data: Dict[str, Any]):
        self.symbol = symbol
        self.action = action  # "BUY", "SELL", "HOLD", "WARMUP"
        self.confidence = confidence
        self.meta_data = meta_data

class MultiAssetPredictor:
    """
    Manages a dictionary of Online Models (one per symbol).
    Performs 'Inference -> Train' loop on every Volume Bar.
    """
    def __init__(self, symbols: List[str]):
        self.symbols = symbols
        self.models_dir = Path("models")
        self.models_dir.mkdir(exist_ok=True)
        
        # 1. State Containers
        self.feature_engineers = {s: OnlineFeatureEngineer(window_size=CONFIG['features']['window_size']) for s in symbols}
        
        # 2. Adaptive Triple Barrier (Per-Symbol Dynamic Configuration)
        tbm_conf = CONFIG['online_learning']['tbm']
        risk_conf = CONFIG.get('risk_management', {})
        
        # AUDIT FIX: Load SL Multiplier from Config (Was Hardcoded 1.5)
        # Now defaults to 2.0 per config update
        risk_mult_conf = float(risk_conf.get('stop_loss_atr_mult', 1.5))
        
        self.labelers = {}
        self.optimized_params = {} # Cache for gates
        
        for s in symbols:
            # Default from Config
            s_risk = risk_mult_conf
            s_reward = tbm_conf.get('barrier_width', 3.0)
            s_horizon = tbm_conf.get('horizon_minutes', 120)
            
            # --- DYNAMIC PARAMETER LOADING ---
            # Try to load optimized R:R and Risk from Research
            params_path = self.models_dir / f"best_params_{s}.json"
            if params_path.exists():
                try:
                    with open(params_path, 'r') as f:
                        bp = json.load(f)
                        self.optimized_params[s] = bp # Store for gate logic
                        
                        # Optuna stores this as a flat key if suggested via suggest_float
                        if 'barrier_width' in bp:
                            s_reward = float(bp['barrier_width'])
                        if 'horizon_minutes' in bp:
                            s_horizon = int(bp['horizon_minutes'])
                        
                        # NEW: Load Risk Param
                        if 'risk_per_trade_percent' in bp:
                            # Store in dict to ensure persistence
                            if s not in self.optimized_params: self.optimized_params[s] = {}
                            self.optimized_params[s]['risk_per_trade_percent'] = float(bp['risk_per_trade_percent'])
                            
                except Exception as e:
                    logger.warning(f"Failed to load optimized params for {s}: {e}")

            # Initialize Labeler with SPECIFIC params
            self.labelers[s] = AdaptiveTripleBarrier(
                horizon_ticks=s_horizon,
                risk_mult=s_risk,
                reward_mult=s_reward,
                drift_threshold=tbm_conf.get('drift_threshold', 1.5)
            )

        # 3. Models (River Ensembles)
        self.models = {}
        self.meta_labelers = {}
        self.calibrators = {}
        
        # 4. Warm-up State
        self.burn_in_counters = {s: 0 for s in symbols}
        self.burn_in_limit = CONFIG['online_learning'].get('burn_in_periods', 200)
        
        # 5. Forensic Stats
        self.rejection_stats = {s: defaultdict(int) for s in symbols}
        self.feature_stats = {s: defaultdict(float) for s in symbols}
        self.bar_counters = {s: 0 for s in symbols}
        self.feature_importance_counter = {s: Counter() for s in symbols}
        
        # 6. Streak Breaker State (Internal Tracking)
        self.consecutive_losses = {s: 0 for s in symbols}
        self.active_signals = {s: deque() for s in symbols} # Track issued signals to match outcomes
        
        # 7. Architecture: Auto-Save Timer
        self.last_save_time = time.time()
        self.save_interval = 300 # 5 Minutes

        # --- Gating Params ---
        self.vol_gate_conf = CONFIG['online_learning'].get('volatility_gate', {})
        self.use_vol_gate = self.vol_gate_conf.get('enabled', True)
        self.min_atr_spread_ratio = self.vol_gate_conf.get('min_atr_spread_ratio', 1.5)
        
        self.spread_map = CONFIG.get('forensic_audit', {}).get('spread_pips', {})
        
        # --- PHOENIX STRATEGY PARAMETERS (DEFAULTS V7.0) ---
        phx_conf = CONFIG.get('phoenix_strategy', {})
        # STRICT THRESHOLDS FROM DOC
        self.default_ker_thresh = 0.30 
        self.default_max_rvol = 6.0
        
        # Friday Guard
        self.friday_entry_cutoff = CONFIG['risk_management'].get('friday_entry_cutoff_hour', 16)
        
        # Fallback Tracking
        self.l2_missing_warned = {s: False for s in symbols}
        self.last_close_prices = {s: 0.0 for s in symbols}
        
        # Timezone for Guard
        tz_str = risk_conf.get('risk_timezone', 'Europe/Prague')
        try:
            self.server_tz = pytz.timezone(tz_str)
        except Exception:
            self.server_tz = pytz.timezone('Europe/Prague')

        # --- REC 1: Dynamic Gate Scaling State ---
        # ADWIN Drift detectors for KER monitoring per symbol
        self.ker_drift_detectors = {s: drift.ADWIN(delta=0.01) for s in symbols}
        self.dynamic_ker_offsets = {s: 0.0 for s in symbols}

        # --- SNIPER PROTOCOL BUFFERS ---
        self.sniper_closes = {s: deque(maxlen=200) for s in symbols} # For SMA 200
        self.sniper_rsi = {s: deque(maxlen=15) for s in symbols}     # For RSI 14

        # Inject Default Data for JPY Basket (Prevents Risk Manager Zeros)
        self._inject_auxiliary_data()

        # Initialize Models & Load State
        self._init_models()
        self._load_state()

    def _init_models(self):
        """
        Initializes the machine learning pipelines using Golden Config hyperparameters.
        """
        conf = CONFIG['online_learning']
        
        metric_map = {
            "LogLoss": metrics.LogLoss(),
            "F1": metrics.F1(),
            "Accuracy": metrics.Accuracy(),
            "ROCAUC": metrics.ROCAUC()
        }
        selected_metric = metric_map.get(conf.get('metric', 'LogLoss'), metrics.LogLoss())
        
        for sym in self.symbols:
            # Base Classifier: ARF
            base_clf = forest.ARFClassifier(
                n_models=conf.get('n_models', 50),
                grace_period=conf['grace_period'],
                delta=conf['delta'],
                split_criterion='gini',
                leaf_prediction='mc',
                max_features=conf.get('max_features', 'sqrt'),
                lambda_value=conf.get('lambda_value', 10),
                metric=selected_metric,
                warning_detector=drift.ADWIN(delta=conf.get('warning_delta', 0.001)),
                drift_detector=drift.ADWIN(delta=conf['delta'])
            )
            
            # Ensemble Wrapper (Bagging)
            self.models[sym] = compose.Pipeline(
                preprocessing.StandardScaler(),
                ensemble.ADWINBaggingClassifier(
                    model=base_clf,
                    n_models=5,
                    seed=42
                )
            )
            
            # Meta Model & Calibrator
            self.meta_labelers[sym] = MetaLabeler()
            self.calibrators[sym] = ProbabilityCalibrator()

    def process_bar(self, symbol: str, bar: VolumeBar, context_data: Dict[str, Any] = None) -> Optional[Signal]:
        """
        Actual entry point called by Engine.
        Executes the Learn-Predict Loop with Project Phoenix V7.5 Logic.
        """
        if symbol not in self.symbols: return None
        
        # --- AUTO SAVE CHECK ---
        if time.time() - self.last_save_time > self.save_interval:
            self.save_state()
            self.last_save_time = time.time()

        fe = self.feature_engineers[symbol]
        labeler = self.labelers[symbol]
        model = self.models[symbol]
        meta_labeler = self.meta_labelers[symbol]
        stats = self.rejection_stats[symbol]
        feat_stats = self.feature_stats[symbol]
        
        self.bar_counters[symbol] += 1
        
        # Ensure we have a price for risk calculations (Live update)
        self.last_close_prices[symbol] = bar.close

        # --- UPDATE SNIPER BUFFERS ---
        self.sniper_closes[symbol].append(bar.close)
        if len(self.sniper_closes[symbol]) > 1:
            delta = self.sniper_closes[symbol][-1] - self.sniper_closes[symbol][-2]
            self.sniper_rsi[symbol].append(delta)

        # Extract Flows (Populated by Aggregator in shared/data.py)
        buy_vol = getattr(bar, 'buy_vol', 0.0)
        sell_vol = getattr(bar, 'sell_vol', 0.0)
        
        # --- RETAIL FALLBACK ---
        if buy_vol == 0 and sell_vol == 0:
            if not self.l2_missing_warned[symbol]:
                logger.warning(f"âš ï¸ {symbol}: Zero Flow Detected. Using Local Tick Rule Fallback.")
                self.l2_missing_warned[symbol] = True
            
            last_price = self.last_close_prices.get(symbol, bar.close)
            if bar.close > last_price:
                buy_vol = bar.volume; sell_vol = 0.0
            elif bar.close < last_price:
                buy_vol = 0.0; sell_vol = bar.volume
            else:
                buy_vol = bar.volume / 2.0; sell_vol = bar.volume / 2.0
        
        # 1. Feature Engineering (Digesting D1/H4 Context)
        features = fe.update(
            price=bar.close,
            timestamp=bar.timestamp.timestamp(),
            volume=bar.volume,
            high=bar.high,
            low=bar.low,
            buy_vol=buy_vol,
            sell_vol=sell_vol,
            context_data=context_data
        )
        
        if features is None: return None
        
        # Extract Key Features
        ker_val = features.get('ker', 0.5)
        parkinson = features.get('parkinson_vol', 0.0)

        # --- REC 1: DYNAMIC GATE SCALING (Drift Detection) ---
        # Update drift detector with current KER
        self.ker_drift_detectors[symbol].update(ker_val)
        
        # If distribution of KER changes significantly (Drift), adjust threshold
        if self.ker_drift_detectors[symbol].drift_detected:
            # Drift implies regime shift. Relax gate temporarily.
            self.dynamic_ker_offsets[symbol] = max(-0.10, self.dynamic_ker_offsets[symbol] - 0.05)
        else:
            # Slowly decay offset back to 0 (Normalization)
            self.dynamic_ker_offsets[symbol] = min(0.0, self.dynamic_ker_offsets[symbol] + 0.001)

        # Update Feature Stats
        alpha = 0.01
        feat_stats['avg_ker'] = (1 - alpha) * feat_stats.get('avg_ker', 0.5) + alpha * features.get('ker', 0.5)
        feat_stats['avg_rvol'] = (1 - alpha) * feat_stats.get('avg_rvol', 1.0) + alpha * features.get('rvol', 1.0)
        
        # --- WARM-UP GATE ---
        if self.burn_in_counters[symbol] < self.burn_in_limit:
            self.burn_in_counters[symbol] += 1
            return Signal(symbol, "WARMUP", 0.0, {})

        # 2. Delayed Training (Label Resolution) & Streak Update
        resolved_labels = labeler.resolve_labels(bar.high, bar.low)
        
        if resolved_labels:
            for (stored_feats, outcome_label, realized_ret) in resolved_labels:
                
                # --- STREAK BREAKER UPDATE ---
                if self.active_signals[symbol]:
                    _ = self.active_signals[symbol].popleft()
                    
                    if outcome_label == 1:
                        self.consecutive_losses[symbol] = 0 # WIN -> Reset Streak
                    elif outcome_label == -1:
                        self.consecutive_losses[symbol] += 1 # LOSS -> Increment Streak
                    # Ignore 0 (Time expiry usually scratch)

                # --- LEARNING ---
                w_pos = CONFIG['online_learning'].get('positive_class_weight', 1.5)
                w_neg = CONFIG['online_learning'].get('negative_class_weight', 1.0)
                
                base_weight = w_pos if outcome_label != 0 else w_neg
                
                # Scale by Profit Magnitude (Log Scale)
                ret_scalar = math.log1p(abs(realized_ret) * 100.0)
                ret_scalar = max(0.5, min(ret_scalar, 5.0))
                
                # --- REC 3: EFFICIENCY-WEIGHTED LEARNING ---
                hist_ker = stored_feats.get('ker', 0.5)
                ker_weight = hist_ker * 2.0  # Scale 0-2 (e.g., 0.8 KER -> 1.6x weight)
                
                final_weight = base_weight * ret_scalar * ker_weight
                
                # Train Primary Model
                model.learn_one(stored_feats, outcome_label, sample_weight=final_weight)
                
                # Double Learn for Positive outcomes
                if outcome_label != 0:
                     model.learn_one(stored_feats, outcome_label, sample_weight=final_weight * 1.5)

                # Update Meta-Labeler
                if outcome_label != 0:
                    meta_labeler.update(stored_feats, primary_action=outcome_label, outcome_pnl=realized_ret)

        # 3. Add CURRENT Bar as new Trade Opportunity
        # Explicitly pass parkinson_vol to labeler for dynamic barrier sizing
        current_atr = features.get('atr', 0.0)
        
        # Inject Parkinson into features
        features['parkinson_vol'] = parkinson 
        
        labeler.add_trade_opportunity(features, bar.close, current_atr, bar.timestamp.timestamp(), parkinson_vol=parkinson)

        # ============================================================
        # 4. PHOENIX STRATEGY V7.5: AGGRESSOR BREAKOUT GATES
        # ============================================================
        
        # Extract Core Indicators
        rvol = features.get('rvol', 1.0)
        aggressor = features.get('aggressor', 0.5)
        adx_val = features.get('adx', 0.0)
        
        # NEW: Anti-Chop Indicator
        choppiness = features.get('choppiness', 50.0)
        
        # Pull Configs (Relaxed)
        phx = CONFIG.get('phoenix_strategy', {})
        max_rvol_thresh = float(phx.get('max_relative_volume', 6.0))
        ker_thresh = float(phx.get('ker_trend_threshold', 0.20)) # Was 0.30
        vol_gate_ratio = float(phx.get('volume_gate_ratio', 1.5)) # Was 2.0
        aggressor_ratio_min = 1.2
        aggressor_pa_thresh = 0.55
        
        # --- FRIDAY GUARD (Timezone Aware) ---
        # Convert UTC bar time to Server Time (Prague)
        server_time = bar.timestamp.astimezone(self.server_tz)
        
        if server_time.weekday() == 4 and server_time.hour >= self.friday_entry_cutoff:
             # Just return HOLD, do not spam stats if it happens often
             return Signal(symbol, "HOLD", 0.0, {"reason": "Friday Entry Guard"})

        # --- CRITICAL FILTER 0: ANTI-CHOP (HARD GATE) ---
        if choppiness > 50.0:
            stats[f"Chop Regime (CHOP {choppiness:.1f} > 50)"] += 1
            return Signal(symbol, "HOLD", 0.0, {"reason": f"Chop Regime (CHOP {choppiness:.1f})"})

        # --- CRITICAL FILTER 1: FUEL GAUGE (HARD GATE) ---
        if rvol < vol_gate_ratio:
            stats[f"Low Fuel (RVol {rvol:.2f} < {vol_gate_ratio})"] += 1
            return Signal(symbol, "HOLD", 0.0, {"reason": "Low Fuel"})

        # --- CRITICAL FILTER 2: VOLUME EXHAUSTION FILTER ---
        if rvol > max_rvol_thresh:
            stats[f"Volume Climax"] += 1
            return Signal(symbol, "HOLD", 0.0, {"reason": "Volume Climax"})
            
        # --- CORRECTED STREAK BREAKER: DYNAMIC EFFICIENCY GATE ---
        # Apply Dynamic Offset from ADWIN (Rec 1)
        effective_ker_thresh = ker_thresh + self.dynamic_ker_offsets[symbol]
        
        streak = self.consecutive_losses[symbol]
        if streak > 0:
            # Soft Breaker: Require cleaner trend if losing
            effective_ker_thresh += min(0.1, streak * 0.02)
        
        # Ensure gate doesn't go below absolute safety minimum of 0.15
        effective_ker_thresh = max(0.15, effective_ker_thresh)

        # --- CRITICAL FILTER 3: EFFICIENCY GATE ---
        ker_val = features.get('ker', 0.5)
        if ker_val < effective_ker_thresh:
            stats[f"Low Efficiency"] += 1
            return Signal(symbol, "HOLD", 0.0, {"reason": f"Low Efficiency (KER {ker_val:.2f} < {effective_ker_thresh:.2f})"})

        # --- CRITICAL FILTER 4: MTF TREND CONTEXT ---
        d1_ema = context_data.get('d1', {}).get('ema200', 0.0) if context_data else 0.0
        d1_trend_up = (bar.close > d1_ema) if d1_ema > 0 else True
        d1_trend_down = (bar.close < d1_ema) if d1_ema > 0 else True
        
        # H4 Alignment
        h4_rsi = context_data.get('h4', {}).get('rsi', 50.0) if context_data else 50.0
        h4_bull = h4_rsi > 50
        h4_bear = h4_rsi < 50
        
        proposed_action = 0
        regime_label = "Chop"

        # --- AGGRESSOR TRIGGER (ORDER FLOW IMBALANCE) ---
        # Logic: Confirm Buy Vol > Sell Vol * 1.2
        safe_sell_vol = sell_vol if sell_vol > 0 else 1.0
        safe_buy_vol = buy_vol if buy_vol > 0 else 1.0
        
        flow_ratio_bull = buy_vol / safe_sell_vol
        flow_ratio_bear = sell_vol / safe_buy_vol
        
        is_bullish_flow = flow_ratio_bull > aggressor_ratio_min
        is_bearish_flow = flow_ratio_bear > aggressor_ratio_min
        
        is_bullish_pa = aggressor > aggressor_pa_thresh
        is_bearish_pa = aggressor < (1.0 - aggressor_pa_thresh)

        # >>> ENTRY LOGIC <<<
        
        # BUY SCENARIO
        if is_bullish_flow and is_bullish_pa and h4_bull:
            if not d1_trend_up:
                stats["Counter-D1 Trend"] += 1
                return Signal(symbol, "HOLD", 0.0, {"reason": "Counter-D1 Trend"})
            proposed_action = 1
            regime_label = "Aggressor-Long"

        # SELL SCENARIO
        elif is_bearish_flow and is_bearish_pa and h4_bear:
            if not d1_trend_down:
                stats["Counter-D1 Trend"] += 1
                return Signal(symbol, "HOLD", 0.0, {"reason": "Counter-D1 Trend"})
            proposed_action = -1
            regime_label = "Aggressor-Short"
            
        else:
            stats["No Trigger"] += 1
            return Signal(symbol, "HOLD", 0.0, {"reason": "No Trigger"})

        # 5. ML Confirmation & Execution
        pred_proba = model.predict_proba_one(features)
        prob_buy = pred_proba.get(1, 0.0)
        prob_sell = pred_proba.get(-1, 0.0)
        
        confidence = prob_buy if proposed_action == 1 else prob_sell
        
        # Meta Labeling
        meta_threshold = CONFIG['online_learning'].get('meta_labeling_threshold', 0.50)
        is_profitable = meta_labeler.predict(
            features,
            proposed_action,
            threshold=meta_threshold
        )

        # --- EXECUTION WITH DYNAMIC CONFIDENCE ---
        min_prob = CONFIG['online_learning'].get('min_calibrated_probability', 0.55)
        
        # Streak Breaker: Increase required confidence
        if streak > 0:
            min_prob += min(0.1, streak * 0.02)

        if confidence < min_prob:
            stats[f"ML Disagreement (Streak: {streak})"] += 1
            return Signal(symbol, "HOLD", confidence, {"reason": f"ML Disagreement (Conf < {min_prob:.2f})"})

        # --- SNIPER PROTOCOL: FINAL FILTER GATE ---
        # Validate Trend (SMA200) and Extension (RSI) before executing
        if not self._check_sniper_filters(symbol, proposed_action, bar.close):
            stats["Sniper Reject"] += 1
            return Signal(symbol, "HOLD", confidence, {"reason": "Sniper Filter Reject"})
        # ------------------------------------------

        if is_profitable:
                action_str = "BUY" if proposed_action == 1 else "SELL"
                
                # Register Signal for Streak Tracking
                self.active_signals[symbol].append(action_str)
                
                # FEATURE IMPORTANCE TRACKING
                imp_feats = []
                imp_feats.append(regime_label)
                if rvol > 2.0: imp_feats.append('High_Fuel')
                if aggressor > 0.6: imp_feats.append('High_Aggressor')
                
                for f in imp_feats:
                    self.feature_importance_counter[symbol][f] += 1
                
                # Retrieve optimized R:R for metadata
                opt_rr = self.labelers[symbol].reward_mult
                opt_risk = self.optimized_params.get(symbol, {}).get('risk_per_trade_percent')

                return Signal(symbol, action_str, confidence, {
                    "meta_ok": True,
                    "volatility": features.get('volatility', 0.001),
                    "atr": current_atr,
                    "ker": ker_val,
                    "parkinson_vol": parkinson,
                    "rvol": rvol,
                    "amihud": features.get('amihud', 0.0),
                    "choppiness": choppiness, # NEW
                    "regime": regime_label,
                    "drivers": imp_feats,
                    "optimized_rr": opt_rr,
                    "risk_percent_override": opt_risk,
                    "pyramid": False
                })
        else:
            stats['Meta Rejected'] += 1
            return Signal(symbol, "HOLD", confidence, {"reason": "Meta Rejected"})

        # Periodic Stats Logging
        if self.bar_counters[symbol] % 250 == 0:
            top_drivers = self.feature_importance_counter[symbol].most_common(3)
            logger.info(f"ðŸ” {symbol} Stats: KER:{feat_stats['avg_ker']:.2f} RVol:{feat_stats['avg_rvol']:.2f} Streak:{self.consecutive_losses[symbol]}")
            logger.info(f"ðŸ” {symbol} Rejections: {dict(stats)}")
            stats.clear()
            
        return Signal(symbol, "HOLD", confidence, {})

    def _check_sniper_filters(self, symbol: str, signal: int, price: float) -> bool:
        """
        SNIPER PROTOCOL (LIVE):
        1. Trend Filter: Trade ONLY if price is above/below SMA 200.
        2. Extension Filter: Don't Buy if RSI > 70, Don't Sell if RSI < 30.
        """
        closes = self.sniper_closes[symbol]
        rsi_buf = self.sniper_rsi[symbol]
        
        if len(closes) < 200:
            return True # Not enough data, allow (or False to be safe)

        # 1. Calculate SMA 200
        sma_200 = sum(closes) / len(closes)
        
        # 2. Calculate Simple RSI (Approximate)
        gains = [x for x in rsi_buf if x > 0]
        losses = [abs(x) for x in rsi_buf if x < 0]
        avg_gain = sum(gains) / 14 if gains else 0
        avg_loss = sum(losses) / 14 if losses else 1e-9
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))

        # 3. Apply Filters
        if signal == 1: # BUY Signal
            if price < sma_200: return False # Counter-Trend
            if rsi > 70: return False        # Overbought
                
        elif signal == -1: # SELL Signal
            if price > sma_200: return False # Counter-Trend
            if rsi < 30: return False        # Oversold
                
        return True

    def _inject_auxiliary_data(self):
        """Injects static approximations ONLY if missing."""
        defaults = {
            "USDJPY": 150.0, "GBPUSD": 1.25, "EURUSD": 1.08,
            "USDCAD": 1.35, "USDCHF": 0.90, "AUDUSD": 0.65, "NZDUSD": 0.60,
            "GBPJPY": 190.0, "EURJPY": 160.0, "AUDJPY": 95.0 # JPY Basket
        }
        for sym, price in defaults.items():
            if sym not in self.last_close_prices or self.last_close_prices[sym] == 0:
                self.last_close_prices[sym] = price

    def save_state(self):
        """Saves models AND Feature Engineers to disk."""
        try:
            for sym in self.symbols:
                with open(self.models_dir / f"river_pipeline_{sym}.pkl", "wb") as f:
                    pickle.dump(self.models[sym], f)
                with open(self.models_dir / f"meta_model_{sym}.pkl", "wb") as f:
                    pickle.dump(self.meta_labelers[sym], f)
                with open(self.models_dir / f"calibrators_{sym}.pkl", "wb") as f:
                    pickle.dump(self.calibrators[sym], f)
                
                # Persist Feature Engineer State
                with open(self.models_dir / f"feature_engineer_{sym}.pkl", "wb") as f:
                    pickle.dump(self.feature_engineers[sym], f)
                    
            logger.info(f"{LogSymbols.DATABASE} Models & State Auto-Saved.")
        except Exception as e:
            logger.error(f"{LogSymbols.ERROR} Failed to save models: {e}")

    def _load_state(self):
        """Loads models AND Feature Engineers from disk."""
        loaded_count = 0
        for sym in self.symbols:
            model_path = self.models_dir / f"river_pipeline_{sym}.pkl"
            meta_path = self.models_dir / f"meta_model_{sym}.pkl"
            fe_path = self.models_dir / f"feature_engineer_{sym}.pkl"
            
            if model_path.exists():
                try:
                    with open(model_path, "rb") as f: self.models[sym] = pickle.load(f)
                    loaded_count += 1
                except Exception: pass
            
            if meta_path.exists():
                try:
                    with open(meta_path, "rb") as f: self.meta_labelers[sym] = pickle.load(f)
                except Exception: pass
            
            if fe_path.exists():
                try:
                    with open(fe_path, "rb") as f: self.feature_engineers[sym] = pickle.load(f)
                    logger.info(f"Loaded Feature State for {sym}")
                except Exception: pass
            
        if loaded_count > 0:
            logger.info(f"{LogSymbols.SUCCESS} Loaded {loaded_count} existing models.")