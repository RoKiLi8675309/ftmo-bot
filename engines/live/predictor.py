# =============================================================================
# FILENAME: engines/live/predictor.py
# ENVIRONMENT: Linux/WSL2 (Python 3.11)
# PATH: engines/live/predictor.py
# DEPENDENCIES: shared, river, numpy
# DESCRIPTION: Online Learning Kernel. Manages ARF models, Feature Engineering,
# Labeling (Adaptive Triple Barrier), and Weighted Learning.
#
# AUDIT REMEDIATION (SNIPER MODE V4):
# 1. WEIGHTING: Balanced weights (1.5/1.5) to value all directional signals.
# 2. FILTERS: Relaxed VPIN/Entropy filters to 0.98 to fix data starvation.
# 3. CONVICTION: Raised min_calibrated_probability to 0.55.
# 4. ALIGNMENT: Matches 'shared/financial/features.py' stationary pipeline.
# =============================================================================
import logging
import pickle
import os
import json
import time
import math
from collections import defaultdict, deque
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

# Third-Party ML Imports
try:
    from river import forest, compose, preprocessing, metrics, drift, linear_model, multioutput
except ImportError:
    print("CRITICAL: 'river' library not found. Install with: pip install river>=0.21.0")
    import sys
    sys.exit(1)

# Shared Imports
from shared import (
    CONFIG,
    LogSymbols,
    OnlineFeatureEngineer,
    AdaptiveTripleBarrier,
    ProbabilityCalibrator,
    enrich_with_d1_data,
    VolumeBar
)

# New Feature Import
from shared.financial.features import MetaLabeler

logger = logging.getLogger("Predictor")

class Signal:
    """
    Represents a trading decision generated by the model.
    """
    def __init__(self, symbol: str, action: str, confidence: float, meta_data: Dict[str, Any]):
        self.symbol = symbol
        self.action = action # "BUY", "SELL", "HOLD", "WARMUP"
        self.confidence = confidence
        self.meta_data = meta_data

class MultiAssetPredictor:
    """
    Manages a dictionary of Online Models (one per symbol).
    Performs 'Inference -> Train' loop on every Volume Bar.
    """
    def __init__(self, symbols: List[str]):
        self.symbols = symbols
        self.models_dir = Path("models")
        self.models_dir.mkdir(exist_ok=True)
        
        # 1. State Containers
        self.feature_engineers = {s: OnlineFeatureEngineer(window_size=CONFIG['features']['window_size']) for s in symbols}
        
        # PHASE 2: Adaptive Triple Barrier (ATR-based)
        tbm_conf = CONFIG['online_learning']['tbm']
        self.labelers = {s: AdaptiveTripleBarrier(
            horizon_ticks=tbm_conf['horizon_minutes'], # Converted to ticks implicitly by usage
            risk_mult=CONFIG['risk_management']['stop_loss_atr_mult'],
            reward_mult=tbm_conf['barrier_width'], # Uses barrier width as TP mult
            drift_threshold=tbm_conf.get('drift_threshold', 1.0) # AUDIT: 1.0 Default
        ) for s in symbols}

        # 2. Models (River ARF)
        self.models = {}
        self.meta_labelers = {} # Secondary Filter
        self.calibrators = {}
        
        # 3. Warm-up State (Phase 2 Requirement)
        self.burn_in_counters = {s: 0 for s in symbols}
        self.burn_in_limit = CONFIG['online_learning'].get('burn_in_periods', 1000)
        
        # 4. Forensic Stats (Refinement #5)
        self.rejection_stats = {s: defaultdict(int) for s in symbols}
        self.bar_counters = {s: 0 for s in symbols}
        
        # 5. Architecture: Auto-Save Timer
        self.last_save_time = time.time()
        self.save_interval = 300 # 5 Minutes

        # Initialize Models
        self._init_models()
        self._load_state()

    def _init_models(self):
        """
        Initializes the machine learning pipelines using Configured hyperparameters.
        """
        conf = CONFIG['online_learning']
        
        for sym in self.symbols:
            # Primary Model: Adaptive Random Forest
            # Optimized for Non-Stationary Data (Drift Detection Enabled)
            self.models[sym] = compose.Pipeline(
                preprocessing.StandardScaler(),
                forest.ARFClassifier(
                    n_models=conf['n_models'],
                    grace_period=conf['grace_period'],
                    delta=conf['delta'],
                    split_criterion='gini',
                    leaf_prediction='mc',
                    max_features='log2',
                    lambda_value=conf['lambda_value'],
                    metric=metrics.F1() # Optimization Target: F1 (Balanced for -1, 0, 1)
                )
            )
            
            # Meta Model: Profitability Filter
            self.meta_labelers[sym] = MetaLabeler()
            self.calibrators[sym] = ProbabilityCalibrator()

    def process_bar(self, symbol: str, bar: VolumeBar, context_d1: Dict[str, Any] = None) -> Optional[Signal]:
        """
        Actual entry point called by Engine.
        Executes the Learn-Predict Loop.
        """
        if symbol not in self.symbols: return None
        
        # --- AUTO SAVE CHECK ---
        if time.time() - self.last_save_time > self.save_interval:
            self.save_state()
            self.last_save_time = time.time()

        fe = self.feature_engineers[symbol]
        labeler = self.labelers[symbol]
        model = self.models[symbol]
        meta_labeler = self.meta_labelers[symbol]
        stats = self.rejection_stats[symbol]
        
        self.bar_counters[symbol] += 1

        # Extract Real Flow
        buy_vol = getattr(bar, 'buy_vol', bar.volume / 2.0)
        sell_vol = getattr(bar, 'sell_vol', bar.volume / 2.0)

        # 1. Feature Engineering (Stationary Pipeline)
        features = fe.update(
            price=bar.close,
            timestamp=bar.timestamp,
            volume=bar.volume,
            high=bar.high,
            low=bar.low,
            buy_vol=buy_vol,
            sell_vol=sell_vol
        )
        
        if features is None: return None
        
        if context_d1:
            features = enrich_with_d1_data(features, context_d1, bar.close)

        # --- PHASE 2: WARM-UP GATE ---
        # We must update features/indicators recursively, but we DO NOT Train or Infer
        # until the burn-in period is complete to establish volatility baselines.
        if self.burn_in_counters[symbol] < self.burn_in_limit:
            self.burn_in_counters[symbol] += 1
            remaining = self.burn_in_limit - self.burn_in_counters[symbol]
            if remaining % 100 == 0:
                logger.info(f"ðŸ”¥ {symbol} Warm-up: {self.burn_in_counters[symbol]}/{self.burn_in_limit}")
            return Signal(symbol, "WARMUP", 0.0, {"remaining": remaining})

        # 2. Delayed Training (Label Resolution via Adaptive Barrier)
        # We check if any PAST trades have closed based on current price action.
        resolved_labels = labeler.resolve_labels(bar.high, bar.low)
        
        if resolved_labels:
            for (stored_feats, outcome_label, realized_ret) in resolved_labels:
                # --- PROFIT WEIGHTED LEARNING (SNIPER MODE) ---
                # We apply higher weights to trades that resulted in significant PnL.
                # This teaches the model to prioritize "Big Moves" over noise.
                
                # AUDIT FIX: Balanced weighting (1.5/1.5) to treat both sides equally
                w_pos = CONFIG['online_learning'].get('positive_class_weight', 1.5)
                w_neg = CONFIG['online_learning'].get('negative_class_weight', 1.5)
                
                # Base weight: Higher for Signals (1/-1), Lower for Noise (0)
                base_weight = w_pos if outcome_label != 0 else 1.0
                
                # Scale by Log Return Magnitude (Stationary scaling)
                # log(1 + 1.0%) = 0.69 (Big Win). log(1 + 0.1%) = 0.09 (Small Win)
                ret_scalar = math.log1p(abs(realized_ret) * 100.0)
                
                # Clamp scalar to avoid exploding weights on anomalies
                ret_scalar = max(0.5, min(ret_scalar, 5.0))
                
                final_weight = base_weight * ret_scalar
                
                # Train Primary Model
                model.learn_one(stored_feats, outcome_label, sample_weight=final_weight)
                
                # Update Meta-Labeler (Did the signal actually make money?)
                if outcome_label != 0:
                    meta_labeler.update(stored_feats, outcome_label, realized_ret)

        # 3. Add CURRENT Bar as new Trade Opportunity (for future labeling)
        current_atr = features.get('atr', 0.0)
        labeler.add_trade_opportunity(features, bar.close, current_atr, bar.timestamp)

        # 4. Inference
        current_pred_action = 0
        try:
            # Forensic Filters (RELAXED to 0.98)
            # We filter OUT extreme entropy/VPIN to avoid trading in chaos,
            # but allow higher noise (0.98) for M5 timeframe.
            entropy_val = features.get('entropy', 0.0)
            entropy_thresh = CONFIG['features'].get('entropy_threshold', 0.98)
            
            if entropy_val > entropy_thresh:
                stats['High Entropy'] += 1
                return Signal(symbol, "HOLD", 0.0, {"reason": f"Max Entropy ({entropy_val:.2f})"})

            vpin_val = features.get('vpin', 0.0)
            vpin_thresh = CONFIG['microstructure'].get('vpin_threshold', 0.98)
            
            if vpin_val > vpin_thresh:
                stats['High VPIN'] += 1
                return Signal(symbol, "HOLD", 0.0, {"reason": f"Toxic VPIN ({vpin_val:.2f})"})

            # Primary Prediction
            pred_class = model.predict_one(features)
            pred_proba = model.predict_proba_one(features)
            
            try:
                current_pred_action = int(pred_class)
            except:
                current_pred_action = 0
                
            # Get Confidence for Specific Action
            # If Model says BUY, we check prob(1). If SELL, prob(-1).
            prob_buy = pred_proba.get(1, 0.0)
            prob_sell = pred_proba.get(-1, 0.0)
            
            confidence = prob_buy if current_pred_action == 1 else prob_sell

            # Meta-Labeling Check
            meta_threshold = CONFIG['online_learning'].get('meta_labeling_threshold', 0.60)
            is_profitable = meta_labeler.predict(
                features,
                current_pred_action,
                threshold=meta_threshold
            )

            # Decision Logic
            # AUDIT FIX: Raised floor to 0.55 to reduce churn
            min_conf = CONFIG['online_learning'].get('min_calibrated_probability', 0.55)
            volatility = features.get('volatility', 0.001)

            # --- STANDARD AI LOGIC ---
            if current_pred_action == 1: # BUY
                if confidence > min_conf:
                    if is_profitable:
                        return Signal(symbol, "BUY", confidence, {"meta_ok": True, "volatility": volatility, "atr": current_atr})
                    else:
                        stats['Meta Rejected'] += 1
                        return Signal(symbol, "HOLD", confidence, {"reason": "Meta Rejected"})
                else:
                    stats['Low Confidence'] += 1
                    return Signal(symbol, "HOLD", confidence, {"reason": f"Low Confidence ({confidence:.2f} < {min_conf})"})
            
            elif current_pred_action == -1: # SELL
                if confidence > min_conf:
                    if is_profitable:
                        return Signal(symbol, "SELL", confidence, {"meta_ok": True, "volatility": volatility, "atr": current_atr})
                    else:
                        stats['Meta Rejected'] += 1
                        return Signal(symbol, "HOLD", confidence, {"reason": "Meta Rejected"})
                else:
                    stats['Low Confidence'] += 1
                    return Signal(symbol, "HOLD", confidence, {"reason": f"Low Confidence ({confidence:.2f} < {min_conf})"})
            
            else:
                stats['Model Predicted 0'] += 1
            
            # Periodic Rejection Log
            if self.bar_counters[symbol] % 100 == 0:
                logger.info(f"ðŸ” {symbol} Rejections (Last 100): {dict(stats)}")

            return Signal(symbol, "HOLD", confidence, {})

        except Exception as e:
            logger.error(f"Inference Error {symbol}: {e}")
            return None

    def save_state(self):
        """Saves models to disk."""
        try:
            for sym in self.symbols:
                with open(self.models_dir / f"river_pipeline_{sym}.pkl", "wb") as f:
                    pickle.dump(self.models[sym], f)
                
                with open(self.models_dir / f"meta_model_{sym}.pkl", "wb") as f:
                    pickle.dump(self.meta_labelers[sym], f)
                
                with open(self.models_dir / f"calibrators_{sym}.pkl", "wb") as f:
                    pickle.dump(self.calibrators[sym], f)
                    
            logger.info(f"{LogSymbols.DATABASE} Models Auto-Saved (5-min Checkpoint).")
        except Exception as e:
            logger.error(f"{LogSymbols.ERROR} Failed to save models: {e}")

    def _load_state(self):
        """Loads models from disk."""
        loaded_count = 0
        for sym in self.symbols:
            model_path = self.models_dir / f"river_pipeline_{sym}.pkl"
            meta_path = self.models_dir / f"meta_model_{sym}.pkl"
            
            if model_path.exists():
                try:
                    with open(model_path, "rb") as f:
                        self.models[sym] = pickle.load(f)
                    loaded_count += 1
                except Exception: pass
            
            if meta_path.exists():
                try:
                    with open(meta_path, "rb") as f:
                        self.meta_labelers[sym] = pickle.load(f)
                except Exception: pass

        if loaded_count > 0:
            logger.info(f"{LogSymbols.SUCCESS} Loaded {loaded_count} existing models.")