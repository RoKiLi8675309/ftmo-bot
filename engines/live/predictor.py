# =============================================================================
# FILENAME: engines/live/predictor.py
# ENVIRONMENT: Linux/WSL2 (Python 3.11)
# PATH: engines/live/predictor.py
# DEPENDENCIES: shared, river, numpy
# DESCRIPTION: Online Learning Kernel. Manages ARF models, Feature Engineering,
# Labeling (TBM), and Signal Calibration.
# AUDIT REMEDIATION (FOREX PLAN):
# 1. PURGED: RandomUnderSampler removed (restores time-series integrity).
# 2. IMBALANCE: Handled via Probability Thresholding (Step 5).
# 3. MEMORY: Relies on Feature Engineering lags rather than sampling.
# =============================================================================
import logging
import pickle
import os
import json
import time
from collections import defaultdict, deque
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

# Third-Party ML Imports
try:
    from river import forest, compose, preprocessing, metrics, drift, linear_model, multioutput
    # AUDIT FIX: Removed RandomUnderSampler import as it destroys time-series autocorrelation
except ImportError:
    print("CRITICAL: 'river' library not found. Install with: pip install river>=0.21.0")
    import sys
    sys.exit(1)

# Shared Imports
from shared import (
    CONFIG,
    LogSymbols,
    OnlineFeatureEngineer,
    StreamingTripleBarrier,
    ProbabilityCalibrator,
    enrich_with_d1_data,
    VolumeBar
)
# New Feature Import
from shared.financial.features import MetaLabeler

logger = logging.getLogger("Predictor")

class Signal:
    """
    Represents a trading decision generated by the model.
    """
    def __init__(self, symbol: str, action: str, confidence: float, meta_data: Dict[str, Any]):
        self.symbol = symbol
        self.action = action  # "BUY", "SELL", "HOLD"
        self.confidence = confidence
        self.meta_data = meta_data

class MultiAssetPredictor:
    """
    Manages a dictionary of Online Models (one per symbol).
    Performs 'Inference -> Train' loop on every Volume Bar.
    """
    def __init__(self, symbols: List[str]):
        self.symbols = symbols
        self.models_dir = Path("models")
        self.models_dir.mkdir(exist_ok=True)
        
        # 1. State Containers
        self.feature_engineers = {s: OnlineFeatureEngineer(window_size=CONFIG['features']['window_size']) for s in symbols}
        self.labelers = {s: StreamingTripleBarrier(
            vol_multiplier=CONFIG['online_learning']['tbm']['barrier_width'],
            barrier_len=50,
            horizon_ticks=CONFIG['online_learning']['tbm']['horizon_minutes']
        ) for s in symbols}

        # 2. Models (River ARF)
        self.models = {}
        self.meta_labelers = {} # Secondary Filter
        self.calibrators = {}

        # 3. Buffer for Training (Store features until label is ready)
        # Store tuple: (features, timestamp, predicted_action)
        self.pending_features = defaultdict(deque)

        # Initialize Models
        self._init_models()
        self._load_state()

    def _init_models(self):
        """
        Initializes the machine learning pipelines.
        AUDIT FIX: Removed RandomUnderSampler. The model now sees the full,
        unbroken time series to preserve autoregressive features.
        """
        conf = CONFIG['online_learning']
        
        for sym in self.symbols:
            # Primary Model: Directional (Buy/Sell/Hold)
            # We use a Pipeline with StandardScaler.
            # Imbalance is now handled by the decision threshold (Step 5), not sampling.
            self.models[sym] = compose.Pipeline(
                preprocessing.StandardScaler(),
                forest.ARFClassifier(
                    n_models=conf['n_models'],
                    grace_period=conf['grace_period'],
                    delta=conf['delta'],
                    split_criterion='gini',
                    leaf_prediction='mc',
                    max_features='log2',
                    lambda_value=conf['lambda_value'],
                    # Metric optimized for Imbalanced classes
                    metric=metrics.GeometricMean()
                )
            )

            # Meta Model: Profitability Filter
            self.meta_labelers[sym] = MetaLabeler()
            self.calibrators[sym] = ProbabilityCalibrator()

    def process_bar(self, symbol: str, bar: VolumeBar, context_d1: Dict[str, Any] = None) -> Optional[Signal]:
        """
        Actual entry point called by Engine.
        """
        if symbol not in self.symbols: return None

        fe = self.feature_engineers[symbol]
        labeler = self.labelers[symbol]
        model = self.models[symbol]
        meta_labeler = self.meta_labelers[symbol]
        
        # Extract Real Flow
        buy_vol = getattr(bar, 'buy_vol', bar.volume / 2.0)
        sell_vol = getattr(bar, 'sell_vol', bar.volume / 2.0)

        # 1. Feature Engineering
        features = fe.update(
            price=bar.close,
            timestamp=bar.timestamp,
            volume=bar.volume,
            buy_vol=buy_vol,
            sell_vol=sell_vol
        )
        
        if features is None: return None

        if context_d1:
            features = enrich_with_d1_data(features, context_d1, bar.close)

        # 2. Delayed Training (Label Resolution)
        resolved_labels = labeler.update(bar.close, bar.timestamp)
        if resolved_labels:
            for (outcome_label, timestamp_in) in resolved_labels:
                # Find matching features
                while self.pending_features[symbol]:
                    stored_data = self.pending_features[symbol][0]
                    # Compatibility check for tuple size
                    if len(stored_data) == 3:
                        stored_feats, stored_ts, predicted_action = stored_data
                    else:
                        stored_feats, stored_ts = stored_data
                        predicted_action = 0 # Default if legacy data

                    if stored_ts < timestamp_in:
                        self.pending_features[symbol].popleft() # Stale
                        continue
                    
                    if stored_ts == timestamp_in:
                        # TRAIN PRIMARY MODEL
                        y_primary = outcome_label # -1, 0, 1
                        model.learn_one(stored_feats, y_primary)

                        # TRAIN META LABELER (PROFITABILITY)
                        # Grok Logic: If primary != 0 and outcome matches, it's PROFIT (1). Else LOSS (0).
                        # Proxy: PnL is positive if direction matches.
                        pnl_proxy = 0.0
                        if predicted_action != 0:
                            if predicted_action == y_primary:
                                pnl_proxy = 1.0 # Winner
                            else:
                                pnl_proxy = -1.0 # Loser
                                
                        meta_labeler.update(stored_feats, predicted_action, pnl_proxy)
                        
                        self.pending_features[symbol].popleft()
                        break
                    
                    if stored_ts > timestamp_in:
                        break

        # 3. Inference
        current_pred_action = 0
        try:
            # Forensic Filters
            entropy_val = features.get('entropy', 0.0)
            if entropy_val > CONFIG['features']['entropy_threshold']:
                return Signal(symbol, "HOLD", 0.0, {"reason": f"Max Entropy ({entropy_val:.2f})"})

            vpin_val = features.get('vpin', 0.0)
            if vpin_val > CONFIG['microstructure']['vpin_threshold']:
                return Signal(symbol, "HOLD", 0.0, {"reason": f"Toxic VPIN ({vpin_val:.2f})"})

            # Primary Prediction
            pred_class = model.predict_one(features)
            pred_proba = model.predict_proba_one(features)
            
            # Helper to safely get confidence
            confidence = pred_proba.get(pred_class, 0.0)
            
            # Convert class to int for storage
            try:
                current_pred_action = int(pred_class)
            except:
                current_pred_action = 0

            # Meta-Labeling Check (Grok Remediation)
            meta_threshold = CONFIG['online_learning'].get('meta_labeling_threshold', 0.60)
            is_profitable = meta_labeler.predict(
                features, 
                current_pred_action, 
                threshold=meta_threshold
            )

            # Store for future training
            self.pending_features[symbol].append((features, bar.timestamp, current_pred_action))

            # Decision Logic (Step 5: Handling Imbalance without Sampling)
            # We strictly enforce a high probability threshold to filter noise.
            min_conf = CONFIG['online_learning'].get('min_calibrated_probability', 0.70) # Raised to 0.70 recommended
            volatility = features.get('volatility', 0.001)

            if current_pred_action != 0:
                if confidence > min_conf:
                    if is_profitable:
                        action_str = "BUY" if current_pred_action == 1 else "SELL"
                        return Signal(symbol, action_str, confidence, {"meta_ok": True, "volatility": volatility})
                    else:
                        # DEBUG: Rejection Log
                        logger.debug(f"ðŸš« {symbol} Meta-Labeler Rejected: Pred={current_pred_action}, Prob={confidence:.2f}")
                        return Signal(symbol, "HOLD", confidence, {"reason": "Meta Rejected"})
                else:
                    return Signal(symbol, "HOLD", confidence, {"reason": f"Low Confidence ({confidence:.2f} < {min_conf})"})
            
            return Signal(symbol, "HOLD", confidence, {})

        except Exception as e:
            logger.error(f"Inference Error {symbol}: {e}")
            return None

    def save_state(self):
        """Saves models to disk."""
        try:
            for sym in self.symbols:
                with open(self.models_dir / f"river_pipeline_{sym}.pkl", "wb") as f:
                    pickle.dump(self.models[sym], f)
                with open(self.models_dir / f"meta_model_{sym}.pkl", "wb") as f:
                    pickle.dump(self.meta_labelers[sym], f)
                with open(self.models_dir / f"calibrators_{sym}.pkl", "wb") as f:
                    pickle.dump(self.calibrators[sym], f)
            logger.info(f"{LogSymbols.DATABASE} Models Auto-Saved.")
        except Exception as e:
            logger.error(f"{LogSymbols.ERROR} Failed to save models: {e}")

    def _load_state(self):
        """Loads models from disk."""
        loaded_count = 0
        for sym in self.symbols:
            model_path = self.models_dir / f"river_pipeline_{sym}.pkl"
            meta_path = self.models_dir / f"meta_model_{sym}.pkl"
            
            if model_path.exists():
                try:
                    with open(model_path, "rb") as f:
                        self.models[sym] = pickle.load(f)
                    loaded_count += 1
                except Exception: pass

            if meta_path.exists():
                try:
                    with open(meta_path, "rb") as f:
                        self.meta_labelers[sym] = pickle.load(f)
                except Exception: pass

        if loaded_count > 0:
            logger.info(f"{LogSymbols.SUCCESS} Loaded {loaded_count} existing models.")